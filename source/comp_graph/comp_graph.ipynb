{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cb8e12",
   "metadata": {},
   "source": [
    "# 计算图\n",
    "\n",
    "计算图是用于神经网络BP算法的一种工具，其基本思想是复合函数的链式求导法则，可简化误差反向传播的计算。\n",
    "\n",
    "## 局部计算节点\n",
    "\n",
    "计算图将神经网络的推理与学习任务分散各个局部计算节点，通过局部节点的计算结果实现神经网络的推理与学习任务。\n",
    "\n",
    "- **加法节点**\n",
    "\n",
    "加法节点的作用是实现以下推理的计算片断，\n",
    "\n",
    "$$\n",
    "x + y = z\n",
    "$$(node-add)\n",
    "\n",
    "误差的反向传播则将$\\frac{\\partial L}{\\partial z} $乘上以下局部计算结果后，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial z}{\\partial x}&=1\\\\\n",
    "\\frac{\\partial z}{\\partial y}&=1\\\\\n",
    "\\end{split}\n",
    "$$(node-add-local-comp)\n",
    "\n",
    "反向传入相应分支，即，$x,y$的各分支分别反向传入$\\frac{\\partial L}{\\partial z}\\times 1$。式{eq}`node-add-local-comp`分别对应各个分支的局部梯度计算结果。加法节点的计算图如下所示：\n",
    "\n",
    ":::{figure-md}\n",
    ":name: fig-comp-graph-node-plus\n",
    "![加法节点](../img/node-plus.svg){width=200px}\n",
    "\n",
    "加法节点\n",
    ":::\n",
    "\n",
    "- **乘法节点**\n",
    "\n",
    "与加法节点类似，实现以下局部计算，\n",
    "\n",
    "$$\n",
    "x*y=z\n",
    "$$(node-mult)\n",
    "\n",
    "误差反向传播则分别将以下结果反向传入对应分支，即$x$分支传入，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}=\\frac{\\partial L}{\\partial z}\\cdot y\n",
    "$$(node-mult-back-x)\n",
    "\n",
    "$y$分支传入，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial y}=\\frac{\\partial L}{\\partial z}\\cdot x\n",
    "$$(node-mult-back-y)\n",
    "\n",
    "乘法节点的计算图如下所示：\n",
    "\n",
    ":::{figure-md} fig-comp-graph-node-times\n",
    "![乘法节点](../img/node-times.svg){width=200px}\n",
    "\n",
    "乘法节点\n",
    ":::\n",
    "\n",
    "图片的引用{ref}`fig-comp-graph-node-times`\n",
    "\n",
    "- **分支节点**\n",
    "\n",
    "分支节点是指相同的值复制后传入各个分支，也称为复制节点。反向传播则是上游传来的梯度之和（与加法节点的运算逻辑正好相反）。\n",
    "\n",
    ":::{figure-md} fig-comp-graph-node-plus\n",
    "![分支节点](../img/node-repeat.svg){width=200px}\n",
    "\n",
    "分支节点示例\n",
    ":::\n",
    "\n",
    "当分支扩展到$N$个节点，则可称为**重复节点**。重复节点的反向传播与分支节点类似，是上游所传的梯度之和。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb40afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 0.22911169 -1.43466507 -0.83761472]]\n",
      "Output y:\n",
      " [[ 0.22911169 -1.43466507 -0.83761472]\n",
      " [ 0.22911169 -1.43466507 -0.83761472]]\n",
      "Gradient dy:\n",
      " [[ 0.46043168  1.54023732  0.17732771]\n",
      " [ 0.62153044  1.17685833 -1.77991773]]\n",
      "Gradient dx:\n",
      " [[ 1.08196212  2.71709565 -1.60259002]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(1, 3)  # 假设的输入数据\n",
    "y = np.repeat(x, 2, axis=0) # 正向传播\n",
    "\n",
    "dy=np.random.randn(2, 3) #假设的梯度\n",
    "dx=np.sum(dy, axis=0, keepdims=True) #梯度传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y) \n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227fe11",
   "metadata": {},
   "source": [
    "- **sum节点**\n",
    "\n",
    "sum节点与重复节点正好相反，推理时其输入为各个分支的和，反向传播时各分支传入的值是其上游值的复制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271a22d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-0.43673587 -0.17054611 -0.31989842]\n",
      " [ 0.70348067  0.88183268 -0.19767539]]\n",
      "Output y:\n",
      " [[ 0.2667448   0.71128656 -0.51757381]]\n",
      "Gradient dy:\n",
      " [[ 0.21866111 -0.21117233 -0.31253189]]\n",
      "Gradient dx:\n",
      " [[ 0.21866111 -0.21117233 -0.31253189]\n",
      " [ 0.21866111 -0.21117233 -0.31253189]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "y = np.sum(x, axis=0, keepdims=True)  # 正向传播\n",
    "dy = np.random.randn(1, 3)  # 假设的梯度\n",
    "dx = np.repeat(dy, 2, axis=0)  # 梯度传播\n",
    "print(\"Input x:\\n\", x)  \n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad906ccf",
   "metadata": {},
   "source": [
    "- **MatMul节点**\n",
    "\n",
    "矩阵乘积节点(假设向量为行向量)，即\n",
    "\n",
    "$$\n",
    "\\pmb{y}_{1\\times H}=\\pmb{x}_{1\\times D}\\pmb{W}_{D\\times H}\n",
    "$$(node-matmul)\n",
    "\n",
    "该计算结点的难点在于反向传播，即以下偏导数(**分子布局**)的计算，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial L}{\\partial \\pmb{x}}_{1\\times D}&=\\frac{\\partial L}{\\partial \\pmb{y}}\\frac{\\partial \\pmb{y}}{\\partial \\pmb{x}}   =\\left[\\frac{\\partial L}{\\pmb{y}}\\right]_{1\\times H}\\left[\\pmb{W}^\\top\\right]_{H\\times D}\\\\\n",
    "\\left[\\frac{\\partial L}{\\partial \\pmb{W}}\\right]_{D\\times H}&=\\frac{\\partial L}{\\partial \\pmb{y}}\\frac{\\partial\\pmb{y}}{\\partial \\pmb{W}}   =\\left[\\pmb{x}^\\top\\right]_{D\\times 1} \\left[\\frac{\\partial L}{\\partial\\pmb{y}}\\right]_{1\\times H}\\\\\n",
    "\\end{split}\n",
    "$$(node-matmul-back)\n",
    "\n",
    "式{eq}`node-matmul-back`的第1个等式容易实现，略过。第2个等式的推导如下：由矩阵乘法定义可知，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_j}{\\partial W_{ik}}=\\left\\{\\begin{array}{ll}x_i,&j==k\\\\ 0,& j\\neq k \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "因此，可以得到以下等式，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{ij}}=\\sum_k \\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial y_j}\\frac{\\partial y_j}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial y_j}\\cdot x_i\n",
    "$$\n",
    "\n",
    "则有梯度如下，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\pmb{W}}=\\left[ \\frac{\\partial L}{\\partial y_j}x_i  \\right]_{ij}=\\left[\\pmb{x}^\\top\\right]_{D\\times 1} \\left[\\frac{\\partial L}{\\partial\\pmb{y}}\\right]_{1\\times H}\n",
    "$$\n",
    "\n",
    "注意：当$\\pmb{x}$是小批量样本时，{eq}`node-matmul-back`形式仍然保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8758672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[0.40429413 0.11323453 0.47863662]\n",
      " [0.42221198 1.18253466 1.19820721]]\n",
      "Weights W:\n",
      " [[-0.62936131 -0.77942967  1.36712125  0.29763984]\n",
      " [-1.20542526  0.33624955  0.37687015  0.69005388]\n",
      " [-0.69869777 -0.7866488   0.10005314 -0.153918  ]]\n",
      "Output y:\n",
      " [[-0.72536519 -0.65356271  0.64328291  0.12480118]\n",
      " [-2.52836575 -0.87402607  1.14276138  0.75725409]]\n",
      "Gradient dy:\n",
      " [[ 0.08156153  0.34855414  1.5969379   2.00391902]\n",
      " [ 0.64648098  0.40669407  1.40479102 -1.35386023]]\n",
      "Gradient dx:\n",
      " [[ 2.45664878  2.00353516 -0.47983712]\n",
      " [ 0.79369737 -1.04734651 -0.42268301]]\n",
      "Gradient dW:\n",
      " [[ 0.30592686  0.3126295   1.23875222  0.23855668]\n",
      " [ 0.77372174  0.5203982   1.84204259 -1.37407382]\n",
      " [ 0.8136565   0.65413454  2.4475837  -0.66305606]]\n"
     ]
    }
   ],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]  #分子布局\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)  # 矩阵乘法\n",
    "        self.x = x  #保存输入，反向传播时使用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)  # 输入梯度\n",
    "        dW = np.dot(self.x.T, dout)  # 权重梯度\n",
    "        self.grads[0][...] = dW  # 更新梯度\n",
    "        return dx\n",
    "    \n",
    "# 测试 MatMul 类\n",
    "W = np.random.randn(3, 4)  # 假设的权重 \n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "matmul = MatMul(W)  \n",
    "y = matmul.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 4)  # 假设的梯度    \n",
    "dx = matmul.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)  \n",
    "print(\"Weights W:\\n\", W)\n",
    "print(\"Output y:\\n\", y)     \n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)     \n",
    "print(\"Gradient dW:\\n\", matmul.grads[0])  # 权重梯度\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebd6e9",
   "metadata": {},
   "source": [
    "## 局部计算的层\n",
    "\n",
    "通过计算图的计算结点，可以实现一些神经网络的常用层。这些层一般都是结点的组合结果。\n",
    "\n",
    "- **sigmoid层**\n",
    "\n",
    "sigmoid层主要由sigmoid函数组成，即\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "y&=\\frac{1}{\\exp(-x)}\\\\\n",
    "\\frac{\\partial y}{\\partial x}&=y(1-y)\n",
    "\\end{split}\n",
    "$$(sigmoid-layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663487d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-1.17101033  1.66884128 -0.98171431]\n",
      " [-1.10026943  0.43641483 -0.04167507]]\n",
      "Output y:\n",
      " [[0.23667241 0.84142127 0.27255176]\n",
      " [0.24968942 0.60740443 0.48958274]]\n",
      "Gradient dy:\n",
      " [[-1.18697953  0.97164569  0.61804125]\n",
      " [ 0.32952121 -0.06370549 -0.19465746]]\n",
      "Gradient dx:\n",
      " [[-0.21443804  0.12964816  0.12253737]\n",
      " [ 0.06173402 -0.01519148 -0.04864324]]\n"
     ]
    }
   ],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []    \n",
    "        self.grads = []\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))  # Sigmoid 函数\n",
    "        self.out = out  # 保存输出，反向传播时使用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1-self.out)  # Sigmoid 的梯度\n",
    "        return dx\n",
    "\n",
    "# 测试 Sigmoid 类\n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "sigmoid = Sigmoid()\n",
    "y = sigmoid.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 3)  # 假设的梯度\n",
    "dx = sigmoid.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)  # Sigmoid 的梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8cbda",
   "metadata": {},
   "source": [
    "- **仿射层(Affine)**\n",
    "\n",
    "Affine层主要实现了线性计算的功能，通过矩阵乘法节点和重复节点完成计算功能。\n",
    "\n",
    "$$\n",
    "\\pmb{z}=\\pmb{x}^\\top\\pmb{W}+\\pmb{b}\n",
    "$$(affine-node)\n",
    "\n",
    ":::{figure-md} fig-affine\n",
    "![仿射节点](../img/node-affine.svg){width=600px}\n",
    "\n",
    "仿射节点\n",
    ":::\n",
    "\n",
    "参见图{ref}`fig-affine`的计算过程。该层的计算代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ab064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-0.68756798  0.43177377 -0.02925057]\n",
      " [-1.36102111  0.45077954  1.55719405]]\n",
      "Weights W:\n",
      " [[ 0.14290845 -0.73001669 -1.59705321  0.71603871]\n",
      " [ 0.63478605 -0.51794666 -0.41526949 -0.31846297]\n",
      " [ 0.09908268 -0.30332353 -0.29725894  1.06398935]]\n",
      "Bias b:\n",
      " [1.43904631 0.13557704 0.30998381 0.18458455]\n",
      "Output y:\n",
      " [[ 1.61197277  0.42274975  1.23745898 -0.47636699]\n",
      " [ 1.68498441  0.42333181  1.83352209  0.72332205]]\n",
      "Gradient dy:\n",
      " [[ 0.47736092  1.962861   -0.90886098 -0.24775618]\n",
      " [-0.83921138 -0.47721892  1.54217898  0.47660766]]\n",
      "Gradient dx:\n",
      " [[-0.09060606 -0.25731185 -0.54152663]\n",
      " [-1.89322497 -1.07774749  0.1102794 ]]\n",
      "Gradient dW:\n",
      " [[ 0.81396632 -0.70009535 -1.47403443 -0.47832386]\n",
      " [-0.1721874   0.63239136  0.30276041  0.10787036]\n",
      " [-1.32077805 -0.80053728  2.42805664  0.74941762]]\n",
      "Gradient db:\n",
      " [-0.36185046  1.48564208  0.633318    0.22885148]\n"
     ]
    }
   ],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None   #反向传播时需要\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dw = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...]=dw\n",
    "        self.grads[1][...]=db\n",
    "        return dx \n",
    "\n",
    "#测试Affine节点\n",
    "W = np.random.randn(3, 4)  # 假设的权重\n",
    "b = np.random.randn(4)  # 假设的偏置    \n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "affine = Affine(W, b)\n",
    "y = affine.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 4)  # 假设的梯度\n",
    "dx = affine.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Weights W:\\n\", W)\n",
    "print(\"Bias b:\\n\", b)\n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)  # 输入梯度\n",
    "print(\"Gradient dW:\\n\", affine.grads[0])  # 权重梯度\n",
    "print(\"Gradient db:\\n\", affine.grads[1])  # 偏置梯度\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1b8e2",
   "metadata": {},
   "source": [
    "- **Softmax损失层**\n",
    "\n",
    "该层主要由softmax函数以及交叉熵损失函数复合而成。softmax函数是指以下函数，\n",
    "\n",
    "$$\n",
    "softmax(\\pmb{x})=\\frac{\\exp(\\pmb{x})}{\\sum_i \\exp(x_i)}\n",
    "$$(softmax-fun-def)\n",
    "\n",
    "交叉熵损失则是指以下损失函数(单个样本one-hot形式)，\n",
    "\n",
    "$$\n",
    "loss(y,t)=\\sum_i t_i\\log y_i\n",
    "$$(corss-entropy-def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee384b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-2.63092393 -0.02650227  0.        ]\n",
      " [-3.61808467 -3.58864891  0.        ]]\n",
      "Labels t:\n",
      " [0 1]\n",
      "Softmax Output y:\n",
      " [[0.03519888 0.47600858 0.48879254]\n",
      " [0.02544789 0.0262081  0.94834402]]\n",
      "Gradient dy:\n",
      " [[-0.48240056  0.23800429  0.24439627]\n",
      " [ 0.01272394 -0.48689595  0.47417201]]\n",
      "Cross Entropy Loss:\n",
      " 3.494210632257319\n",
      "Gradient dx:\n",
      " [[-0.48240056  0.23800429  0.24439627]\n",
      " [ 0.01272394 -0.48689595  0.47417201]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:  # 如果是二维数组\n",
    "        x -= np.max(x, axis=1, keepdims=True)  # 减去每行的最大值\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)  # softmax计算\n",
    "    else:  # 如果是一维数组\n",
    "        x -= np.max(x)  # 减去最大值\n",
    "        y = np.exp(x) / np.sum(np.exp(x))  # softmax计算\n",
    "    return y\n",
    "\n",
    "def cross_entropy_loss(y, t):\n",
    "    if y.ndim == 1:  # 如果是向量\n",
    "        y = y.reshape(1, -1)  # 转换为二维数组\n",
    "        t = t.reshape(1, -1)  # 转换为二维数组\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)  # 如果t是one-hot编码，转换为类别索引\n",
    "    batch_size = y.shape[0]  # 获取批大小\n",
    "    loss = -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size  # 计算交叉熵损失\n",
    "    return loss\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None  # 保存softmax输出\n",
    "        self.t = None  # 保存标签\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t  # 保存标签\n",
    "        self.y = softmax(x)  # 计算softmax输出\n",
    "\n",
    "        # 在监督标签为one-hot向量的情况下，转换为正确解标签的索引\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        loss = cross_entropy_loss(self.y, self.t)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.y.shape[0]  # 获取批大小\n",
    "        dx = self.y.copy()  # 复制softmax输出\n",
    "        dx[np.arange(batch_size), self.t] -= 1  # 减去正确类别的梯度\n",
    "        dx *= dout\n",
    "        dx /= batch_size  # 平均化梯度\n",
    "        return dx\n",
    "    \n",
    "# 测试 SoftmaxWithLoss 类\n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "t = np.array([0, 1])  # 假设的标签  \n",
    "softmax_loss = SoftmaxWithLoss()\n",
    "y = softmax_loss.forward(x, t)  # 正向传播\n",
    "dy = softmax_loss.backward()  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Labels t:\\n\", t)\n",
    "print(\"Softmax Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)  # softmax的梯度\n",
    "# 测试 SoftmaxWithLoss 类的交叉熵损失\n",
    "loss = cross_entropy_loss(y, t)  # 计算交叉熵损失\n",
    "print(\"Cross Entropy Loss:\\n\", loss)  # 输出交叉熵损失\n",
    "# 测试 SoftmaxWithLoss 类的梯度\n",
    "print(\"Gradient dx:\\n\", dy)  # 输出梯度\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
