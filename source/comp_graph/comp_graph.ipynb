{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cb8e12",
   "metadata": {},
   "source": [
    "# 计算图\n",
    "\n",
    "计算图是用于神经网络BP算法的一种工具，其基本思想是复合函数的链式求导法则，可简化误差反向传播的计算。\n",
    "\n",
    "## 局部计算节点\n",
    "\n",
    "计算图将神经网络的推理与学习任务分散各个局部计算节点，通过局部节点的计算结果实现神经网络的推理与学习任务。\n",
    "\n",
    "- **加法节点**\n",
    "\n",
    "加法节点的作用是实现以下推理的计算片断，\n",
    "\n",
    "$$\n",
    "x + y = z\n",
    "$$(node-add)\n",
    "\n",
    "误差的反向传播则将$\\frac{\\partial L}{\\partial z} $乘上以下局部计算结果后，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial z}{\\partial x}&=1\\\\\n",
    "\\frac{\\partial z}{\\partial y}&=1\\\\\n",
    "\\end{split}\n",
    "$$(node-add-local-comp)\n",
    "\n",
    "反向传入相应分支，即，$x,y$的各分支分别反向传入$\\frac{\\partial L}{\\partial z}\\times 1$。式{eq}`node-add-local-comp`分别对应各个分支的局部梯度计算结果。加法节点的计算图如下所示：\n",
    "\n",
    ":::{figure-md}\n",
    ":name: fig-comp-graph-node-plus\n",
    "![加法节点](../img/node-plus.svg){width=200px}\n",
    "\n",
    "加法节点\n",
    ":::\n",
    "\n",
    "- **乘法节点**\n",
    "\n",
    "与加法节点类似，实现以下局部计算，\n",
    "\n",
    "$$\n",
    "x*y=z\n",
    "$$(node-mult)\n",
    "\n",
    "误差反向传播则分别将以下结果反向传入对应分支，即$x$分支传入，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}=\\frac{\\partial L}{\\partial z}\\cdot y\n",
    "$$(node-mult-back-x)\n",
    "\n",
    "$y$分支传入，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial y}=\\frac{\\partial L}{\\partial z}\\cdot x\n",
    "$$(node-mult-back-y)\n",
    "\n",
    "乘法节点的计算图如下所示：\n",
    "\n",
    ":::{figure-md} fig-comp-graph-node-times\n",
    "![乘法节点](../img/node-times.svg){width=200px}\n",
    "\n",
    "乘法节点\n",
    ":::\n",
    "\n",
    "图片的引用{ref}`fig-comp-graph-node-times`\n",
    "\n",
    "- **分支节点**\n",
    "\n",
    "分支节点是指相同的值复制后传入各个分支，也称为复制节点。反向传播则是上游传来的梯度之和（与加法节点的运算逻辑正好相反）。\n",
    "\n",
    ":::{figure-md} fig-comp-graph-node-plus\n",
    "![分支节点](../img/node-repeat.svg){width=200px}\n",
    "\n",
    "分支节点示例\n",
    ":::\n",
    "\n",
    "当分支扩展到$N$个节点，则可称为**重复节点**。重复节点的反向传播与分支节点类似，是上游所传的梯度之和。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb40afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[0.72206891 0.35389271 1.72719994]]\n",
      "Output y:\n",
      " [[0.72206891 0.35389271 1.72719994]\n",
      " [0.72206891 0.35389271 1.72719994]]\n",
      "Gradient dy:\n",
      " [[-0.67843465  0.65692653 -1.63123009]\n",
      " [-0.97966934 -0.71100144 -0.25300468]]\n",
      "Gradient dx:\n",
      " [[-1.65810399 -0.05407491 -1.88423478]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(1, 3)  # 假设的输入数据\n",
    "y = np.repeat(x, 2, axis=0) # 正向传播\n",
    "\n",
    "dy=np.random.randn(2, 3) #假设的梯度\n",
    "dx=np.sum(dy, axis=0, keepdims=True) #梯度传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y) \n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227fe11",
   "metadata": {},
   "source": [
    "- **sum节点**\n",
    "\n",
    "sum节点与重复节点正好相反，推理时其输入为各个分支的和，反向传播时各分支传入的值是其上游值的复制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "271a22d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 0.92489881  0.86625853 -0.54744188]\n",
      " [-0.0619725  -0.62391338 -0.17605427]]\n",
      "Output y:\n",
      " [[ 0.86292631  0.24234514 -0.72349616]]\n",
      "Gradient dy:\n",
      " [[-0.96189216 -0.72897876 -0.0355408 ]]\n",
      "Gradient dx:\n",
      " [[-0.96189216 -0.72897876 -0.0355408 ]\n",
      " [-0.96189216 -0.72897876 -0.0355408 ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "y = np.sum(x, axis=0, keepdims=True)  # 正向传播\n",
    "dy = np.random.randn(1, 3)  # 假设的梯度\n",
    "dx = np.repeat(dy, 2, axis=0)  # 梯度传播\n",
    "print(\"Input x:\\n\", x)  \n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad906ccf",
   "metadata": {},
   "source": [
    "- **MatMul节点**\n",
    "\n",
    "矩阵乘积节点(假设向量为行向量)，即\n",
    "\n",
    "$$\n",
    "\\pmb{y}_{1\\times H}=\\pmb{x}_{1\\times D}\\pmb{W}_{D\\times H}\n",
    "$$(node-matmul)\n",
    "\n",
    "该计算结点的难点在于反向传播，即以下偏导数(**分子布局**)的计算，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial L}{\\partial \\pmb{x}}_{1\\times D}&=\\frac{\\partial L}{\\partial \\pmb{y}}\\frac{\\partial \\pmb{y}}{\\partial \\pmb{x}}   =\\left[\\frac{\\partial L}{\\pmb{y}}\\right]_{1\\times H}\\left[\\pmb{W}^\\top\\right]_{H\\times D}\\\\\n",
    "\\left[\\frac{\\partial L}{\\partial \\pmb{W}}\\right]_{D\\times H}&=\\frac{\\partial L}{\\partial \\pmb{y}}\\frac{\\partial\\pmb{y}}{\\partial \\pmb{W}}   =\\left[\\pmb{x}^\\top\\right]_{D\\times 1} \\left[\\frac{\\partial L}{\\partial\\pmb{y}}\\right]_{1\\times H}\\\\\n",
    "\\end{split}\n",
    "$$(node-matmul-back)\n",
    "\n",
    "式{eq}`node-matmul-back`的第1个等式容易实现，略过。第2个等式的推导如下：由矩阵乘法定义可知，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_j}{\\partial W_{ik}}=\\left\\{\\begin{array}{ll}x_i,&j==k\\\\ 0,& j\\neq k \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "因此，可以得到以下等式，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{ij}}=\\sum_k \\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial y_j}\\frac{\\partial y_j}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial y_j}\\cdot x_i\n",
    "$$\n",
    "\n",
    "则有梯度如下，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\pmb{W}}=\\left[ \\frac{\\partial L}{\\partial y_j}x_i  \\right]_{ij}=\\left[\\pmb{x}^\\top\\right]_{D\\times 1} \\left[\\frac{\\partial L}{\\partial\\pmb{y}}\\right]_{1\\times H}\n",
    "$$\n",
    "\n",
    "注意：当$\\pmb{x}$是小批量样本时，{eq}`node-matmul-back`形式仍然保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8758672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-0.93687019  0.57828754 -0.76800081]\n",
      " [-0.00399834  0.1000363  -1.1926997 ]]\n",
      "Weights W:\n",
      " [[-0.93569731  0.72740693  2.95627284 -0.19767411]\n",
      " [ 0.78800105  0.3317538   0.27264821 -0.77995306]\n",
      " [-1.71889265  0.94648079 -0.74407137  0.08367462]]\n",
      "Output y:\n",
      " [[ 2.65242905 -1.2165348  -2.04052743 -0.33010432]\n",
      " [ 2.1326927  -1.09858836  0.90290823 -0.17703194]]\n",
      "Gradient dy:\n",
      " [[-0.21584035  0.67554835 -0.10841518 -2.37497773]\n",
      " [ 0.36791339  0.42988391  0.54364351 -0.63800321]]\n",
      "Gradient dx:\n",
      " [[ 0.84232656  1.87684524  0.8923432 ]\n",
      " [ 1.70172023  1.07836775 -0.68342101]]\n",
      "Gradient dW:\n",
      " [[ 0.20074334 -0.63461993  0.09939727  2.2275968 ]\n",
      " [-0.08801309  0.43366518 -0.00831106 -1.4372435 ]\n",
      " [-0.27304463 -1.03154408 -0.56514051  2.58493105]]\n"
     ]
    }
   ],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]  #分子布局\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)  # 矩阵乘法\n",
    "        self.x = x  #保存输入，反向传播时使用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)  # 输入梯度\n",
    "        dW = np.dot(self.x.T, dout)  # 权重梯度\n",
    "        self.grads[0][...] = dW  # 更新梯度\n",
    "        return dx\n",
    "    \n",
    "# 测试 MatMul 类\n",
    "W = np.random.randn(3, 4)  # 假设的权重 \n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "matmul = MatMul(W)  \n",
    "y = matmul.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 4)  # 假设的梯度    \n",
    "dx = matmul.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)  \n",
    "print(\"Weights W:\\n\", W)\n",
    "print(\"Output y:\\n\", y)     \n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)     \n",
    "print(\"Gradient dW:\\n\", matmul.grads[0])  # 权重梯度\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebd6e9",
   "metadata": {},
   "source": [
    "## 局部计算的层\n",
    "\n",
    "通过计算图的计算结点，可以实现一些神经网络的常用层。这些层一般都是结点的组合结果。\n",
    "\n",
    "### **sigmoid层**\n",
    "\n",
    "sigmoid层主要由sigmoid函数组成，即\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "y&=\\frac{1}{\\exp(-x)}\\\\\n",
    "\\frac{\\partial y}{\\partial x}&=y(1-y)\n",
    "\\end{split}\n",
    "$$(sigmoid-layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "663487d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 1.43282435 -0.87674304 -0.63521218]\n",
      " [-0.97507988 -1.85462779  0.61775703]]\n",
      "Output y:\n",
      " [[0.807341   0.29385315 0.34632963]\n",
      " [0.27386913 0.13533046 0.64970825]]\n",
      "Gradient dy:\n",
      " [[ 1.29730034 -0.64475725  0.06031784]\n",
      " [-0.47752643 -0.1826718   1.30658829]]\n",
      "Gradient dx:\n",
      " [[ 0.20178405 -0.13378937  0.01365508]\n",
      " [-0.09496321 -0.02137555  0.29736308]]\n"
     ]
    }
   ],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []    \n",
    "        self.grads = []\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))  # Sigmoid 函数\n",
    "        self.out = out  # 保存输出，反向传播时使用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1-self.out)  # Sigmoid 的梯度\n",
    "        return dx\n",
    "\n",
    "# 测试 Sigmoid 类\n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "sigmoid = Sigmoid()\n",
    "y = sigmoid.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 3)  # 假设的梯度\n",
    "dx = sigmoid.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)  # Sigmoid 的梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3d5980",
   "metadata": {},
   "source": [
    "### **ReLU层**\n",
    "\n",
    "ReLU (Rectified Linear Unit)函数定义如下，\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) =\n",
    "\\begin{cases}\n",
    "x, & \\text{if } x > 0; \\\\\n",
    "0, & \\text{if } x \\leq 0.\n",
    "\\end{cases}\n",
    "$$(relu-def)\n",
    "\n",
    "易知，该函数的导数为，\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\text{ReLU}(x)=\n",
    "\\begin{cases}\n",
    "1,&\\text{if } x>0;\\\\\n",
    "0,&\\text{if } x\\le 0.\n",
    "\\end{cases}\n",
    "$$(relu-der-def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71cca572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 1.00959427 -0.15736295  0.26379541]\n",
      " [-0.03848728 -1.39110164 -0.4967722 ]]\n",
      "Output y:\n",
      " [[1.00959427 0.         0.26379541]\n",
      " [0.         0.         0.        ]]\n",
      "Gradient dy:\n",
      " [[ 0.31260195  0.         -0.36205015]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient dx:\n",
      " [[ 0.31260195  0.         -0.36205015]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "# 测试 Relu 类\n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "relu = Relu()\n",
    "y = relu.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 3)  # 假设的梯度\n",
    "dx = relu.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)  # Relu 的梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8cbda",
   "metadata": {},
   "source": [
    "### **仿射层(Affine)**\n",
    "\n",
    "Affine层主要实现了线性计算的功能，通过矩阵乘法节点和重复节点完成计算功能。\n",
    "\n",
    "$$\n",
    "\\pmb{z}=\\pmb{x}^\\top\\pmb{W}+\\pmb{b}\n",
    "$$(affine-node)\n",
    "\n",
    ":::{figure-md} fig-affine\n",
    "![仿射节点](../img/node-affine.svg){width=600px}\n",
    "\n",
    "仿射节点\n",
    ":::\n",
    "\n",
    "参见图{ref}`fig-affine`的计算过程。该层的计算代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66ab064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-1.02898513  1.13416245  0.85302068]\n",
      " [ 0.24062443  0.96097782 -0.85473967]]\n",
      "Weights W:\n",
      " [[ 0.08850005  0.3438311   0.64733261 -0.11949433]\n",
      " [-0.70266012  0.48296786  0.57096931  0.48416589]\n",
      " [-2.09054459 -1.11171277 -0.28543242  0.17613605]]\n",
      "Bias b:\n",
      " [ 2.1261547   2.39785713 -1.43789332  0.54487446]\n",
      "Output y:\n",
      " [[-0.54511903  1.64351007 -1.69989675  1.36720282]\n",
      " [ 3.25908058  3.89493769 -0.48947002  0.83084341]]\n",
      "Gradient dy:\n",
      " [[-0.40635683  0.94765271 -0.52173287 -0.29886113]\n",
      " [-2.03194024 -0.23013739  0.04897851 -1.28197821]]\n",
      "Gradient dx:\n",
      " [[-0.01215261  0.30062472 -0.10773129]\n",
      " [-0.07406069  0.72388952  4.26392572]]\n",
      "Gradient dW:\n",
      " [[-7.07993375e-02 -1.03049723e+00  5.48640789e-01 -9.51621894e-04]\n",
      " [-2.41352416e+00  8.53635197e-01 -5.44662573e-01 -1.57090970e+00]\n",
      " [ 1.39014915e+00  1.00507491e+00 -4.86912796e-01  8.40822907e-01]]\n",
      "Gradient db:\n",
      " [-2.43829707  0.71751532 -0.47275436 -1.58083934]\n"
     ]
    }
   ],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None   #反向传播时需要\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dw = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...]=dw\n",
    "        self.grads[1][...]=db\n",
    "        return dx \n",
    "\n",
    "#测试Affine节点\n",
    "W = np.random.randn(3, 4)  # 假设的权重\n",
    "b = np.random.randn(4)  # 假设的偏置    \n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "affine = Affine(W, b)\n",
    "y = affine.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 4)  # 假设的梯度\n",
    "dx = affine.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Weights W:\\n\", W)\n",
    "print(\"Bias b:\\n\", b)\n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)  # 输入梯度\n",
    "print(\"Gradient dW:\\n\", affine.grads[0])  # 权重梯度\n",
    "print(\"Gradient db:\\n\", affine.grads[1])  # 偏置梯度\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1b8e2",
   "metadata": {},
   "source": [
    "### **Softmax损失层**\n",
    "\n",
    "该层主要由softmax函数以及交叉熵损失函数复合而成。softmax函数是指以下函数，\n",
    "\n",
    "$$\n",
    "\\sigma(\\pmb{x})=\\frac{\\exp(\\pmb{x})}{\\sum_i \\exp(x_i)}\n",
    "$$(softmax-fun-def)\n",
    "\n",
    "交叉熵损失则是指以下损失函数(单个样本one-hot形式)，\n",
    "\n",
    "$$\n",
    "loss(y,t)=-\\sum_i t_i\\log y_i\n",
    "$$(corss-entropy-def)\n",
    "\n",
    "式{eq}`softmax-fun-def`的偏导数为（分子布局），\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\pmb{x}) }{\\partial \\pmb{x}}= \\begin{bmatrix} \\frac{\\partial \\sigma_1}{\\partial x_1} & \\frac{\\partial \\sigma_1}{\\partial x_2} &\\cdots & \\frac{\\partial \\sigma_1}{\\partial x_n} \\\\ \\frac{\\partial \\sigma_2}{\\partial x_1} & \\frac{\\partial \\sigma_2}{\\partial x_2} &\\cdots & \\frac{\\partial \\sigma_2}{\\partial x_n} \\\\ \n",
    "\\vdots & \\vdots &\\ddots &\\vdots \\\\\n",
    "\\frac{\\partial \\sigma_n}{\\partial x_1} & \\frac{\\partial \\sigma_n}{\\partial x_2} &\\cdots & \\frac{\\partial \\sigma_n}{\\partial x_n} \\\\\\end{bmatrix}\n",
    "$$(derivative-softmax)\n",
    "\n",
    "其中， $\\sigma_i = \\frac{e^{x_i}}{\\sum_{k}e^{x_k}}$。令$S=\\sum_{k}e^{x_k}, \\delta_{ik}=\\{1 (i=k), 0(i\\neq k)\\}$，则项$\\frac{\\partial \\sigma_i}{\\partial x_k}$可通过以下计算得到，即\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial \\sigma_i}{\\partial x_k} &= e^{x_i}\\cdot \\delta_{ik}\\cdot\\frac1S - \\frac{e^{x_i}}{S^2}\\cdot\\frac{\\partial S}{\\partial x_k}\\\\\n",
    "&=\\sigma_i(\\delta_{ik}-\\sigma_k)\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "由$\\delta_{ik}$可知，矩阵只有对角线元素为$\\sigma_i-\\sigma_i\\sigma_k$，其余元素均为$-\\sigma_i\\sigma_k$，因此，式{eq}`derivative-softmax`可成以下形式，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\pmb{x}) }{\\partial \\pmb{x}} =\\text{diag}(\\pmb{\\sigma})-\\pmb{\\sigma}\\pmb{\\sigma}^\\top\n",
    "$$(derivative-softmax-detail)\n",
    "\n",
    "若采用交叉熵做为损失函数，则可推导出$\\frac{\\partial L}{\\partial \\pmb{x}}$(分子布局)如下，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial L}{\\partial \\pmb{x}} &= \\frac{\\partial L}{\\partial \\sigma(\\pmb{x})}\\frac{\\partial \\sigma(\\pmb{x})}{\\partial \\pmb{x}}\\\\\n",
    "&=\\left[\\frac{-t_1}{\\sigma(\\pmb{x})_1}, \\frac{-t_2}{\\sigma(\\pmb{x})_2}, ..., \\frac{-t_n}{\\sigma(\\pmb{x})_n} \\right]\\left(\\text{diag}(\\pmb{\\sigma})-\\pmb{\\sigma}\\pmb{\\sigma}^\\top\\right)\\\\\n",
    "&=[\\sigma_1-t_1,\\sigma_2-t_2,...,\\sigma_n-t_n]\n",
    "\\end{split}\n",
    "$$(derivative-softmax-with-loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dee384b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-0.01879012 -0.54829851  0.        ]\n",
      " [-0.84352563  0.         -0.07835929]]\n",
      "Labels t:\n",
      " [0 1]\n",
      "Softmax Output y:\n",
      " [[0.38345585 0.225815   0.39072915]\n",
      " [0.18268511 0.42466031 0.39265458]]\n",
      "Gradient dy:\n",
      " [[-0.30827208  0.1129075   0.19536457]\n",
      " [ 0.09134255 -0.28766984  0.19632729]]\n",
      "Cross Entropy Loss:\n",
      " 0.9074979969849499\n",
      "Gradient dx:\n",
      " [[-0.30827208  0.1129075   0.19536457]\n",
      " [ 0.09134255 -0.28766984  0.19632729]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:  # 如果是二维数组\n",
    "        x -= np.max(x, axis=1, keepdims=True)  # 减去每行的最大值\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)  # softmax计算\n",
    "    else:  # 如果是一维数组\n",
    "        x -= np.max(x)  # 减去最大值\n",
    "        y = np.exp(x) / np.sum(np.exp(x))  # softmax计算\n",
    "    return y\n",
    "\n",
    "def cross_entropy_loss(y, t):\n",
    "    if y.ndim == 1:  # 如果是向量\n",
    "        y = y.reshape(1, -1)  # 转换为二维数组\n",
    "        t = t.reshape(1, -1)  # 转换为二维数组\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)  # 如果t是one-hot编码，转换为类别索引\n",
    "    batch_size = y.shape[0]  # 获取批大小\n",
    "    loss = -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size  # 计算交叉熵损失\n",
    "    return loss\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None  # 保存softmax输出\n",
    "        self.t = None  # 保存标签\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t  # 保存标签\n",
    "        self.y = softmax(x)  # 计算softmax输出\n",
    "\n",
    "        # 在监督标签为one-hot向量的情况下，转换为正确解标签的索引\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        loss = cross_entropy_loss(self.y, self.t)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.y.shape[0]  # 获取批大小\n",
    "        dx = self.y.copy()  # 复制softmax输出\n",
    "        dx[np.arange(batch_size), self.t] -= 1  # 减去正确类别的梯度\n",
    "        dx *= dout\n",
    "        dx /= batch_size  # 平均化梯度\n",
    "        return dx\n",
    "    \n",
    "# 测试 SoftmaxWithLoss 类\n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "t = np.array([0, 1])  # 假设的标签  \n",
    "softmax_loss = SoftmaxWithLoss()\n",
    "y = softmax_loss.forward(x, t)  # 正向传播\n",
    "dy = softmax_loss.backward()  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Labels t:\\n\", t)\n",
    "print(\"Softmax Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)  # softmax的梯度\n",
    "# 测试 SoftmaxWithLoss 类的交叉熵损失\n",
    "loss = cross_entropy_loss(y, t)  # 计算交叉熵损失\n",
    "print(\"Cross Entropy Loss:\\n\", loss)  # 输出交叉熵损失\n",
    "# 测试 SoftmaxWithLoss 类的梯度\n",
    "print(\"Gradient dx:\\n\", dy)  # 输出梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fdfbbc",
   "metadata": {},
   "source": [
    "## 一个完整的网络\n",
    "\n",
    "下面将使用以上所设计的模块，构建一个2层的示例前向网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3a432bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 0.54576423  0.70829806 -1.39423678 -0.74337296]\n",
      " [-2.10609531  0.58867088  0.19073156 -0.86751311]]\n",
      "Labels t:\n",
      " [0 1]\n",
      "Loss:\n",
      " [[0.33326675 0.3334218  0.33331145]\n",
      " [0.33335621 0.33329482 0.33334898]]\n",
      "Accuracy:\n",
      " 0.0\n",
      "Gradient W1:\n",
      " [[-0.00462424  0.0005273  -0.00812862  0.0011283   0.        ]\n",
      " [ 0.00129251  0.00068433  0.00407598  0.00146432  0.        ]\n",
      " [ 0.00041878 -0.00134706 -0.00208202 -0.00288242  0.        ]\n",
      " [-0.00190475 -0.00071822 -0.00537711 -0.00153684  0.        ]]\n",
      "Gradient b1:\n",
      " [0.00219565 0.00096616 0.00649818 0.00206738 0.        ]\n",
      "Gradient W2:\n",
      " [[ 2.62461166e-03 -5.24916640e-03  2.62455474e-03]\n",
      " [-1.52762858e-02  7.63940701e-03  7.63687876e-03]\n",
      " [-2.78548305e-03  7.54273022e-05  2.71005575e-03]\n",
      " [-5.25436777e-03  2.62761869e-03  2.62674908e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "Gradient b2:\n",
      " [-0.16668852 -0.16664169  0.33333021]\n"
     ]
    }
   ],
   "source": [
    "# 二层前向网络\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class SimpleNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化网络参数\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size) * weight_init_std\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size) * weight_init_std \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 所有层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # 前向传播\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 反向传播\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward()\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout) \n",
    "        # 梯度\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].grads[0]\n",
    "        grads['b1'] = self.layers['Affine1'].grads[1]   \n",
    "        grads['W2'] = self.layers['Affine2'].grads[0]\n",
    "        grads['b2'] = self.layers['Affine2'].grads[1]\n",
    "        return grads\n",
    "\n",
    "# 测试 SimpleNet 类\n",
    "input_size = 4  # 输入层大小    \n",
    "hidden_size = 5  # 隐藏层大小\n",
    "output_size = 3  # 输出层大小\n",
    "net = SimpleNet(input_size, hidden_size, output_size)  # 创建网络\n",
    "x = np.random.randn(2, input_size)  # 假设的输入数据\n",
    "t = np.array([0, 1])  # 假设的标签\n",
    "loss = net.loss(x, t)  # 计算损失\n",
    "accuracy = net.accuracy(x, t)  # 计算准确率\n",
    "grads = net.gradient(x, t)  # 计算梯度\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Labels t:\\n\", t)\n",
    "print(\"Loss:\\n\", loss)  # 输出损失\n",
    "print(\"Accuracy:\\n\", accuracy)  # 输出准确率\n",
    "print(\"Gradient W1:\\n\", grads['W1'])  # 输出第一层权重梯度\n",
    "print(\"Gradient b1:\\n\", grads['b1'])  # 输出第一层偏置梯度\n",
    "print(\"Gradient W2:\\n\", grads['W2'])  # 输出第二层权重梯度\n",
    "print(\"Gradient b2:\\n\", grads['b2'])  # 输出第二层偏置梯度\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762aa9c",
   "metadata": {},
   "source": [
    "### 前向网络的BP训练\n",
    "\n",
    "使用BP算法对上述自定义网络进行训练，寻找最优参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "249eaae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Accuracy: 0.1106, Test Accuracy: 0.1078\n",
      "Iteration 600, Train Accuracy: 0.9051, Test Accuracy: 0.9064\n",
      "Iteration 1200, Train Accuracy: 0.9223, Test Accuracy: 0.9245\n",
      "Iteration 1800, Train Accuracy: 0.9362, Test Accuracy: 0.9351\n",
      "Iteration 2400, Train Accuracy: 0.9445, Test Accuracy: 0.9417\n",
      "Iteration 3000, Train Accuracy: 0.9524, Test Accuracy: 0.9497\n",
      "Iteration 3600, Train Accuracy: 0.9584, Test Accuracy: 0.9550\n",
      "Iteration 4200, Train Accuracy: 0.9625, Test Accuracy: 0.9562\n",
      "Iteration 4800, Train Accuracy: 0.9661, Test Accuracy: 0.9620\n",
      "Iteration 5400, Train Accuracy: 0.9683, Test Accuracy: 0.9636\n",
      "Iteration 6000, Train Accuracy: 0.9708, Test Accuracy: 0.9640\n",
      "Iteration 6600, Train Accuracy: 0.9725, Test Accuracy: 0.9659\n",
      "Iteration 7200, Train Accuracy: 0.9735, Test Accuracy: 0.9659\n",
      "Iteration 7800, Train Accuracy: 0.9760, Test Accuracy: 0.9672\n",
      "Iteration 8400, Train Accuracy: 0.9770, Test Accuracy: 0.9683\n",
      "Iteration 9000, Train Accuracy: 0.9781, Test Accuracy: 0.9676\n",
      "Iteration 9600, Train Accuracy: 0.9789, Test Accuracy: 0.9695\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = SimpleNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000  # 迭代次数\n",
    "train_size = x_train.shape[0]  # 训练集大小\n",
    "batch_size = 100  # 批大小\n",
    "learning_rate = 0.1  # 学习率\n",
    "train_loss_list = []  # 存储训练损失\n",
    "train_acc_list = []  # 存储训练准确率\n",
    "test_acc_list = []  # 存储测试准确率\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)  # 每个epoch的迭代次数\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)  # 随机选择批次\n",
    "    x_batch = x_train[batch_mask]  # 获取批次数据\n",
    "    t_batch = t_train[batch_mask]  # 获取批次标签\n",
    "\n",
    "    # 计算梯度\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grads[key]\n",
    "\n",
    "    # 记录训练损失\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 每个epoch计算一次准确率\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)  # 训练集准确率\n",
    "        test_acc = network.accuracy(x_test, t_test)  # 测试集准确率\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Iteration {i}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6ca7f",
   "metadata": {},
   "source": [
    "## 优化器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "745973de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    '''\n",
    "    随机梯度下降法（Stochastic Gradient Descent）\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr  # 学习率\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]  # 更新参数\n",
    "\n",
    "\n",
    "class Momentum:\n",
    "    '''\n",
    "    Momentum SGD\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]\n",
    "\n",
    "class AdaGrad:\n",
    "    '''\n",
    "    AdaGrad\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] += grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "    '''\n",
    "    RMSprop\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] *= self.decay_rate\n",
    "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df4d6a",
   "metadata": {},
   "source": [
    "## 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3f4ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "def remove_duplicates(params, grads):\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 在共享权重的情况下\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 加上梯度\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 在作为转置矩阵共享权重的情况下（weight tying）\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "        if not find_flg: break\n",
    "    return params, grads\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []  # 存储损失值\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval= eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0.0\n",
    "        loss_count = 0\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            idx = np.random.permutation(data_size)\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters * batch_size:(iters + 1) * batch_size]\n",
    "                batch_t = t[iters * batch_size:(iters + 1) * batch_size]\n",
    "\n",
    "                loss = model.forward(batch_x, batch_t)  # 正向传播\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicates(model.params, model.grads)  # 获取参数和梯度\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "                # 评价\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
