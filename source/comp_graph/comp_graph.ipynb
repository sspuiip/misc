{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cb8e12",
   "metadata": {},
   "source": [
    "# 计算图\n",
    "\n",
    "计算图是用于神经网络BP算法的一种工具，其基本思想是复合函数的链式求导法则，可简化误差反向传播的计算。\n",
    "\n",
    "## 局部计算节点\n",
    "\n",
    "计算图将神经网络的推理与学习任务分散各个局部计算节点，通过局部节点的计算结果实现神经网络的推理与学习任务。\n",
    "\n",
    "- **加法节点**\n",
    "\n",
    "加法节点的作用是实现以下推理的计算片断，\n",
    "\n",
    "$$\n",
    "x + y = z\n",
    "$$(node-add)\n",
    "\n",
    "误差的反向传播则将$\\frac{\\partial L}{\\partial z} $乘上以下局部计算结果后，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial z}{\\partial x}&=1\\\\\n",
    "\\frac{\\partial z}{\\partial y}&=1\\\\\n",
    "\\end{split}\n",
    "$$(node-add-local-comp)\n",
    "\n",
    "反向传入相应分支，即，$x,y$的各分支分别反向传入$\\frac{\\partial L}{\\partial z}\\times 1$。式{eq}`node-add-local-comp`分别对应各个分支的局部梯度计算结果。加法节点的计算图如下所示：\n",
    "\n",
    ":::{figure-md}\n",
    ":name: fig-comp-graph-node-plus\n",
    "![加法节点](../img/node-plus.svg){width=200px}\n",
    "\n",
    "加法节点\n",
    ":::\n",
    "\n",
    "- **乘法节点**\n",
    "\n",
    "与加法节点类似，实现以下局部计算，\n",
    "\n",
    "$$\n",
    "x*y=z\n",
    "$$(node-mult)\n",
    "\n",
    "误差反向传播则分别将以下结果反向传入对应分支，即$x$分支传入，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}=\\frac{\\partial L}{\\partial z}\\cdot y\n",
    "$$(node-mult-back-x)\n",
    "\n",
    "$y$分支传入，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial y}=\\frac{\\partial L}{\\partial z}\\cdot x\n",
    "$$(node-mult-back-y)\n",
    "\n",
    "乘法节点的计算图如下所示：\n",
    "\n",
    ":::{figure-md} fig-comp-graph-node-times\n",
    "![乘法节点](../img/node-times.svg){width=200px}\n",
    "\n",
    "乘法节点\n",
    ":::\n",
    "\n",
    "图片的引用{ref}`fig-comp-graph-node-times`\n",
    "\n",
    "- **分支节点**\n",
    "\n",
    "分支节点是指相同的值复制后传入各个分支，也称为复制节点。反向传播则是上游传来的梯度之和（与加法节点的运算逻辑正好相反）。\n",
    "\n",
    ":::{figure-md} fig-comp-graph-node-plus\n",
    "![分支节点](../img/node-repeat.svg){width=200px}\n",
    "\n",
    "分支节点示例\n",
    ":::\n",
    "\n",
    "当分支扩展到$N$个节点，则可称为**重复节点**。重复节点的反向传播与分支节点类似，是上游所传的梯度之和。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb40afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 0.22911169 -1.43466507 -0.83761472]]\n",
      "Output y:\n",
      " [[ 0.22911169 -1.43466507 -0.83761472]\n",
      " [ 0.22911169 -1.43466507 -0.83761472]]\n",
      "Gradient dy:\n",
      " [[ 0.46043168  1.54023732  0.17732771]\n",
      " [ 0.62153044  1.17685833 -1.77991773]]\n",
      "Gradient dx:\n",
      " [[ 1.08196212  2.71709565 -1.60259002]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(1, 3)  # 假设的输入数据\n",
    "y = np.repeat(x, 2, axis=0) # 正向传播\n",
    "\n",
    "dy=np.random.randn(2, 3) #假设的梯度\n",
    "dx=np.sum(dy, axis=0, keepdims=True) #梯度传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y) \n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227fe11",
   "metadata": {},
   "source": [
    "- **sum节点**\n",
    "\n",
    "sum节点与重复节点正好相反，推理时其输入为各个分支的和，反向传播时各分支传入的值是其上游值的复制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271a22d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-0.43673587 -0.17054611 -0.31989842]\n",
      " [ 0.70348067  0.88183268 -0.19767539]]\n",
      "Output y:\n",
      " [[ 0.2667448   0.71128656 -0.51757381]]\n",
      "Gradient dy:\n",
      " [[ 0.21866111 -0.21117233 -0.31253189]]\n",
      "Gradient dx:\n",
      " [[ 0.21866111 -0.21117233 -0.31253189]\n",
      " [ 0.21866111 -0.21117233 -0.31253189]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "y = np.sum(x, axis=0, keepdims=True)  # 正向传播\n",
    "dy = np.random.randn(1, 3)  # 假设的梯度\n",
    "dx = np.repeat(dy, 2, axis=0)  # 梯度传播\n",
    "print(\"Input x:\\n\", x)  \n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad906ccf",
   "metadata": {},
   "source": [
    "- **MatMul节点**\n",
    "\n",
    "矩阵乘积节点(假设向量为行向量)，即\n",
    "\n",
    "$$\n",
    "\\pmb{y}_{1\\times H}=\\pmb{x}_{1\\times D}\\pmb{W}_{D\\times H}\n",
    "$$(node-matmul)\n",
    "\n",
    "该计算结点的难点在于反向传播，即以下偏导数(**分子布局**)的计算，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial L}{\\partial \\pmb{x}}_{1\\times D}&=\\frac{\\partial L}{\\partial \\pmb{y}}\\frac{\\partial \\pmb{y}}{\\partial \\pmb{x}}   =\\left[\\frac{\\partial L}{\\pmb{y}}\\right]_{1\\times H}\\left[\\pmb{W}^\\top\\right]_{H\\times D}\\\\\n",
    "\\left[\\frac{\\partial L}{\\partial \\pmb{W}}\\right]_{D\\times H}&=\\frac{\\partial L}{\\partial \\pmb{y}}\\frac{\\partial\\pmb{y}}{\\partial \\pmb{W}}   =\\left[\\pmb{x}^\\top\\right]_{D\\times 1} \\left[\\frac{\\partial L}{\\partial\\pmb{y}}\\right]_{1\\times H}\\\\\n",
    "\\end{split}\n",
    "$$(node-matmul-back)\n",
    "\n",
    "式{eq}`node-matmul-back`的第1个等式容易实现，略过。第2个等式的推导如下：由矩阵乘法定义可知，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_j}{\\partial W_{ik}}=\\left\\{\\begin{array}{ll}x_i,&j==k\\\\ 0,& j\\neq k \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "因此，可以得到以下等式，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{ij}}=\\sum_k \\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial y_j}\\frac{\\partial y_j}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial y_j}\\cdot x_i\n",
    "$$\n",
    "\n",
    "则有梯度如下，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\pmb{W}}=\\left[ \\frac{\\partial L}{\\partial y_j}x_i  \\right]_{ij}=\\left[\\pmb{x}^\\top\\right]_{D\\times 1} \\left[\\frac{\\partial L}{\\partial\\pmb{y}}\\right]_{1\\times H}\n",
    "$$\n",
    "\n",
    "注意：当$\\pmb{x}$是小批量样本时，{eq}`node-matmul-back`形式仍然保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8758672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[0.40429413 0.11323453 0.47863662]\n",
      " [0.42221198 1.18253466 1.19820721]]\n",
      "Weights W:\n",
      " [[-0.62936131 -0.77942967  1.36712125  0.29763984]\n",
      " [-1.20542526  0.33624955  0.37687015  0.69005388]\n",
      " [-0.69869777 -0.7866488   0.10005314 -0.153918  ]]\n",
      "Output y:\n",
      " [[-0.72536519 -0.65356271  0.64328291  0.12480118]\n",
      " [-2.52836575 -0.87402607  1.14276138  0.75725409]]\n",
      "Gradient dy:\n",
      " [[ 0.08156153  0.34855414  1.5969379   2.00391902]\n",
      " [ 0.64648098  0.40669407  1.40479102 -1.35386023]]\n",
      "Gradient dx:\n",
      " [[ 2.45664878  2.00353516 -0.47983712]\n",
      " [ 0.79369737 -1.04734651 -0.42268301]]\n",
      "Gradient dW:\n",
      " [[ 0.30592686  0.3126295   1.23875222  0.23855668]\n",
      " [ 0.77372174  0.5203982   1.84204259 -1.37407382]\n",
      " [ 0.8136565   0.65413454  2.4475837  -0.66305606]]\n"
     ]
    }
   ],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]  #分子布局\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)  # 矩阵乘法\n",
    "        self.x = x  #保存输入，反向传播时使用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)  # 输入梯度\n",
    "        dW = np.dot(self.x.T, dout)  # 权重梯度\n",
    "        self.grads[0][...] = dW  # 更新梯度\n",
    "        return dx\n",
    "    \n",
    "# 测试 MatMul 类\n",
    "W = np.random.randn(3, 4)  # 假设的权重 \n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "matmul = MatMul(W)  \n",
    "y = matmul.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 4)  # 假设的梯度    \n",
    "dx = matmul.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)  \n",
    "print(\"Weights W:\\n\", W)\n",
    "print(\"Output y:\\n\", y)     \n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)     \n",
    "print(\"Gradient dW:\\n\", matmul.grads[0])  # 权重梯度\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebd6e9",
   "metadata": {},
   "source": [
    "## 局部计算的层\n",
    "\n",
    "通过计算图的计算结点，可以实现一些神经网络的常用层。这些层一般都是结点的组合结果。\n",
    "\n",
    "- **sigmoid层**\n",
    "\n",
    "sigmoid层主要由sigmoid函数组成，即\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "y&=\\frac{1}{\\exp(-x)}\\\\\n",
    "\\frac{\\partial y}{\\partial x}&=y(1-y)\n",
    "\\end{split}\n",
    "$$(sigmoid-layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663487d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-1.17101033  1.66884128 -0.98171431]\n",
      " [-1.10026943  0.43641483 -0.04167507]]\n",
      "Output y:\n",
      " [[0.23667241 0.84142127 0.27255176]\n",
      " [0.24968942 0.60740443 0.48958274]]\n",
      "Gradient dy:\n",
      " [[-1.18697953  0.97164569  0.61804125]\n",
      " [ 0.32952121 -0.06370549 -0.19465746]]\n",
      "Gradient dx:\n",
      " [[-0.21443804  0.12964816  0.12253737]\n",
      " [ 0.06173402 -0.01519148 -0.04864324]]\n"
     ]
    }
   ],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []    \n",
    "        self.grads = []\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))  # Sigmoid 函数\n",
    "        self.out = out  # 保存输出，反向传播时使用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1-self.out)  # Sigmoid 的梯度\n",
    "        return dx\n",
    "\n",
    "# 测试 Sigmoid 类\n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "sigmoid = Sigmoid()\n",
    "y = sigmoid.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 3)  # 假设的梯度\n",
    "dx = sigmoid.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)  # Sigmoid 的梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8cbda",
   "metadata": {},
   "source": [
    "- **仿射层(Affine)**\n",
    "\n",
    "Affine层主要实现了线性计算的功能，通过矩阵乘法节点和重复节点完成计算功能。\n",
    "\n",
    "$$\n",
    "\\pmb{z}=\\pmb{x}^\\top\\pmb{W}+\\pmb{b}\n",
    "$$(affine-node)\n",
    "\n",
    ":::{figure-md} fig-affine\n",
    "![仿射节点](../img/node-affine.svg){width=600px}\n",
    "\n",
    "仿射节点\n",
    ":::\n",
    "\n",
    "参见图{ref}`fig-affine`的计算过程。该层的计算代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ab064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-0.68756798  0.43177377 -0.02925057]\n",
      " [-1.36102111  0.45077954  1.55719405]]\n",
      "Weights W:\n",
      " [[ 0.14290845 -0.73001669 -1.59705321  0.71603871]\n",
      " [ 0.63478605 -0.51794666 -0.41526949 -0.31846297]\n",
      " [ 0.09908268 -0.30332353 -0.29725894  1.06398935]]\n",
      "Bias b:\n",
      " [1.43904631 0.13557704 0.30998381 0.18458455]\n",
      "Output y:\n",
      " [[ 1.61197277  0.42274975  1.23745898 -0.47636699]\n",
      " [ 1.68498441  0.42333181  1.83352209  0.72332205]]\n",
      "Gradient dy:\n",
      " [[ 0.47736092  1.962861   -0.90886098 -0.24775618]\n",
      " [-0.83921138 -0.47721892  1.54217898  0.47660766]]\n",
      "Gradient dx:\n",
      " [[-0.09060606 -0.25731185 -0.54152663]\n",
      " [-1.89322497 -1.07774749  0.1102794 ]]\n",
      "Gradient dW:\n",
      " [[ 0.81396632 -0.70009535 -1.47403443 -0.47832386]\n",
      " [-0.1721874   0.63239136  0.30276041  0.10787036]\n",
      " [-1.32077805 -0.80053728  2.42805664  0.74941762]]\n",
      "Gradient db:\n",
      " [-0.36185046  1.48564208  0.633318    0.22885148]\n"
     ]
    }
   ],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None   #反向传播时需要\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dw = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...]=dw\n",
    "        self.grads[1][...]=db\n",
    "        return dx \n",
    "\n",
    "#测试Affine节点\n",
    "W = np.random.randn(3, 4)  # 假设的权重\n",
    "b = np.random.randn(4)  # 假设的偏置    \n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "affine = Affine(W, b)\n",
    "y = affine.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 4)  # 假设的梯度\n",
    "dx = affine.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Weights W:\\n\", W)\n",
    "print(\"Bias b:\\n\", b)\n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)  # 输入梯度\n",
    "print(\"Gradient dW:\\n\", affine.grads[0])  # 权重梯度\n",
    "print(\"Gradient db:\\n\", affine.grads[1])  # 偏置梯度\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1b8e2",
   "metadata": {},
   "source": [
    "- **Softmax损失层**\n",
    "\n",
    "该层主要由softmax函数以及交叉熵损失函数复合而成。softmax函数是指以下函数，\n",
    "\n",
    "$$\n",
    "softmax(\\pmb{x})=\\frac{\\exp(\\pmb{x})}{\\sum_i \\exp(x_i)}\n",
    "$$(softmax-fun-def)\n",
    "\n",
    "交叉熵损失则是指以下损失函数(单个样本one-hot形式)，\n",
    "\n",
    "$$\n",
    "loss(y,t)=\\sum_i t_i\\log y_i\n",
    "$$(corss-entropy-def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee384b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-2.63092393 -0.02650227  0.        ]\n",
      " [-3.61808467 -3.58864891  0.        ]]\n",
      "Labels t:\n",
      " [0 1]\n",
      "Softmax Output y:\n",
      " [[0.03519888 0.47600858 0.48879254]\n",
      " [0.02544789 0.0262081  0.94834402]]\n",
      "Gradient dy:\n",
      " [[-0.48240056  0.23800429  0.24439627]\n",
      " [ 0.01272394 -0.48689595  0.47417201]]\n",
      "Cross Entropy Loss:\n",
      " 3.494210632257319\n",
      "Gradient dx:\n",
      " [[-0.48240056  0.23800429  0.24439627]\n",
      " [ 0.01272394 -0.48689595  0.47417201]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:  # 如果是二维数组\n",
    "        x -= np.max(x, axis=1, keepdims=True)  # 减去每行的最大值\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)  # softmax计算\n",
    "    else:  # 如果是一维数组\n",
    "        x -= np.max(x)  # 减去最大值\n",
    "        y = np.exp(x) / np.sum(np.exp(x))  # softmax计算\n",
    "    return y\n",
    "\n",
    "def cross_entropy_loss(y, t):\n",
    "    if y.ndim == 1:  # 如果是向量\n",
    "        y = y.reshape(1, -1)  # 转换为二维数组\n",
    "        t = t.reshape(1, -1)  # 转换为二维数组\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)  # 如果t是one-hot编码，转换为类别索引\n",
    "    batch_size = y.shape[0]  # 获取批大小\n",
    "    loss = -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size  # 计算交叉熵损失\n",
    "    return loss\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None  # 保存softmax输出\n",
    "        self.t = None  # 保存标签\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t  # 保存标签\n",
    "        self.y = softmax(x)  # 计算softmax输出\n",
    "\n",
    "        # 在监督标签为one-hot向量的情况下，转换为正确解标签的索引\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        loss = cross_entropy_loss(self.y, self.t)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.y.shape[0]  # 获取批大小\n",
    "        dx = self.y.copy()  # 复制softmax输出\n",
    "        dx[np.arange(batch_size), self.t] -= 1  # 减去正确类别的梯度\n",
    "        dx *= dout\n",
    "        dx /= batch_size  # 平均化梯度\n",
    "        return dx\n",
    "    \n",
    "# 测试 SoftmaxWithLoss 类\n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "t = np.array([0, 1])  # 假设的标签  \n",
    "softmax_loss = SoftmaxWithLoss()\n",
    "y = softmax_loss.forward(x, t)  # 正向传播\n",
    "dy = softmax_loss.backward()  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Labels t:\\n\", t)\n",
    "print(\"Softmax Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)  # softmax的梯度\n",
    "# 测试 SoftmaxWithLoss 类的交叉熵损失\n",
    "loss = cross_entropy_loss(y, t)  # 计算交叉熵损失\n",
    "print(\"Cross Entropy Loss:\\n\", loss)  # 输出交叉熵损失\n",
    "# 测试 SoftmaxWithLoss 类的梯度\n",
    "print(\"Gradient dx:\\n\", dy)  # 输出梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6ca7f",
   "metadata": {},
   "source": [
    "## 优化器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745973de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    '''\n",
    "    随机梯度下降法（Stochastic Gradient Descent）\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr  # 学习率\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]  # 更新参数\n",
    "\n",
    "\n",
    "class Momentum:\n",
    "    '''\n",
    "    Momentum SGD\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]\n",
    "\n",
    "class AdaGrad:\n",
    "    '''\n",
    "    AdaGrad\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] += grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "    '''\n",
    "    RMSprop\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] *= self.decay_rate\n",
    "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df4d6a",
   "metadata": {},
   "source": [
    "## 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "def remove_duplicates(params, grads):\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 在共享权重的情况下\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 加上梯度\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 在作为转置矩阵共享权重的情况下（weight tying）\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "        if not find_flg: break\n",
    "    return params, grads\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []  # 存储损失值\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval= eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0.0\n",
    "        loss_count = 0\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            idx = np.random.permutation(data_size)\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters * batch_size:(iters + 1) * batch_size]\n",
    "                batch_t = t[iters * batch_size:(iters + 1) * batch_size]\n",
    "\n",
    "                loss = model.forward(batch_x, batch_t)  # 正向传播\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicates(model.params, model.grads)  # 获取参数和梯度\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "                # 评价\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
