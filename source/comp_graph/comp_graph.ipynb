{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cb8e12",
   "metadata": {},
   "source": [
    "# 计算图\n",
    "\n",
    "计算图是用于神经网络BP算法的一种工具，其基本思想是复合函数的链式求导法则，可简化误差反向传播的计算。\n",
    "\n",
    "## 局部计算节点\n",
    "\n",
    "计算图将神经网络的推理与学习任务分散各个局部计算节点，通过局部节点的计算结果实现神经网络的推理与学习任务。\n",
    "\n",
    "- **加法节点**\n",
    "\n",
    "加法节点的作用是实现以下推理的计算片断，\n",
    "\n",
    "$$\n",
    "x + y = z\n",
    "$$(node-add)\n",
    "\n",
    "误差的反向传播则将$\\frac{\\partial L}{\\partial z} $乘上以下局部计算结果后，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial z}{\\partial x}&=1\\\\\n",
    "\\frac{\\partial z}{\\partial y}&=1\\\\\n",
    "\\end{split}\n",
    "$$(node-add-local-comp)\n",
    "\n",
    "反向传入相应分支，即，$x,y$的各分支分别反向传入$\\frac{\\partial L}{\\partial z}\\times 1$。式{eq}`node-add-local-comp`分别对应各个分支的局部梯度计算结果。\n",
    "\n",
    "- **乘法节点**\n",
    "\n",
    "与加法节点类似，实现以下局部计算，\n",
    "\n",
    "$$\n",
    "x*y=z\n",
    "$$(node-mult)\n",
    "\n",
    "误差反向传播则分别将以下结果反向传入对应分支，即$x$分支传入，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}=\\frac{\\partial L}{\\partial z}\\cdot y\n",
    "$$(node-mult-back-x)\n",
    "\n",
    "$y$分支传入，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial y}=\\frac{\\partial L}{\\partial z}\\cdot x\n",
    "$$(node-mult-back-y)\n",
    "\n",
    "- **分支节点**\n",
    "\n",
    "分支节点是指相同的值复制后传入各个分支，也称为复制节点。反向传播则是上游传来的梯度之和。当分支扩展到$N$个节点，则可称为**重复节点**。重复节点的反向传播与分支节点类似，是上游所传的梯度之和。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb40afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 1.19485093  1.66341524 -1.59975205]]\n",
      "Output y:\n",
      " [[ 1.19485093  1.66341524 -1.59975205]\n",
      " [ 1.19485093  1.66341524 -1.59975205]]\n",
      "Gradient dy:\n",
      " [[-0.06655354  0.16201839 -0.75654729]\n",
      " [-1.0893431   0.20312229 -0.33846025]]\n",
      "Gradient dx:\n",
      " [[-1.15589664  0.36514068 -1.09500754]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(1, 3)  # 假设的输入数据\n",
    "y = np.repeat(x, 2, axis=0) # 正向传播\n",
    "\n",
    "dy=np.random.randn(2, 3) #假设的梯度\n",
    "dx=np.sum(dy, axis=0, keepdims=True) #梯度传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y) \n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227fe11",
   "metadata": {},
   "source": [
    "- **sum节点**\n",
    "\n",
    "sum节点与重复节点正好相反，推理时其输入为各个分支的和，反向传播时各分支传入的值是其上游值的复制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "271a22d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 1.57649617 -0.46032091  1.06130582]\n",
      " [ 1.02455351 -0.34278245  0.93201846]]\n",
      "Output y:\n",
      " [[ 2.60104969 -0.80310336  1.99332428]]\n",
      "Gradient dy:\n",
      " [[-1.7463373  -0.01989783  0.34527752]]\n",
      "Gradient dx:\n",
      " [[-1.7463373  -0.01989783  0.34527752]\n",
      " [-1.7463373  -0.01989783  0.34527752]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "y = np.sum(x, axis=0, keepdims=True)  # 正向传播\n",
    "dy = np.random.randn(1, 3)  # 假设的梯度\n",
    "dx = np.repeat(dy, 2, axis=0)  # 梯度传播\n",
    "print(\"Input x:\\n\", x)  \n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad906ccf",
   "metadata": {},
   "source": [
    "- **MatMul节点**\n",
    "\n",
    "矩阵乘积节点(假设向量为行向量)，即\n",
    "\n",
    "$$\n",
    "\\pmb{y}_{1\\times H}=\\pmb{x}_{1\\times D}\\pmb{W}_{D\\times H}\n",
    "$$(node-matmul)\n",
    "\n",
    "该计算结点的难点在于反向传播，即以下偏导数(**分子布局**)的计算，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial L}{\\partial \\pmb{x}}_{1\\times D}&=\\frac{\\partial L}{\\partial \\pmb{y}}\\frac{\\partial \\pmb{y}}{\\partial \\pmb{x}}   =\\left[\\frac{\\partial L}{\\pmb{y}}\\right]_{1\\times H}\\left[\\pmb{W}^\\top\\right]_{H\\times D}\\\\\n",
    "\\left[\\frac{\\partial L}{\\partial \\pmb{W}}\\right]_{D\\times H}&=\\frac{\\partial L}{\\partial \\pmb{y}}\\frac{\\partial\\pmb{y}}{\\partial \\pmb{W}}   =\\left[\\pmb{x}^\\top\\right]_{D\\times 1} \\left[\\frac{\\partial L}{\\partial\\pmb{y}}\\right]_{1\\times H}\\\\\n",
    "\\end{split}\n",
    "$$(node-matmul-back)\n",
    "\n",
    "式{eq}`node-matmul-back`的第1个等式容易实现，略过。第2个等式的推导如下：由矩阵乘法定义可知，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_j}{\\partial W_{ik}}=\\left\\{\\begin{array}{ll}x_i,&j==k\\\\ 0,& j\\neq k \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "因此，可以得到以下等式，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{ij}}=\\sum_k \\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial y_j}\\frac{\\partial y_j}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial y_j}\\cdot x_i\n",
    "$$\n",
    "\n",
    "则有梯度如下，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\pmb{W}}=\\left[ \\frac{\\partial L}{\\partial y_j}x_i  \\right]_{ij}=\\left[\\pmb{x}^\\top\\right]_{D\\times 1} \\left[\\frac{\\partial L}{\\partial\\pmb{y}}\\right]_{1\\times H}\n",
    "$$\n",
    "\n",
    "注意：当$\\pmb{x}$是小批量样本时，{eq}`node-matmul-back`形式仍然保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8758672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-0.38569833 -0.07324896 -0.58229009]\n",
      " [-0.96699073 -0.56693264  0.80395212]]\n",
      "Weights W:\n",
      " [[ 0.13311926 -1.15183989  0.06628773 -0.69977611]\n",
      " [-0.45366951 -0.94971094  0.3307429  -0.43096151]\n",
      " [-1.43208512 -0.14871347  2.30186971  0.64263319]]\n",
      "Output y:\n",
      " [[ 0.81577592  0.60042243 -1.39014957 -0.07272898]\n",
      " [-1.02285291  1.53268212  1.59898447  1.43764948]]\n",
      "Gradient dy:\n",
      " [[ 2.26454874  0.3221979   0.40649511 -0.59622509]\n",
      " [ 0.05977523  1.20000979  2.68998884  0.31412483]]\n",
      "Gradient dx:\n",
      " [[ 0.37450437 -0.94195614 -2.73839696]\n",
      " [-1.41576571 -0.41246162  6.12981002]]\n",
      "Gradient dW:\n",
      " [[-0.93123475 -1.28466953 -2.75797876 -0.07379278]\n",
      " [-0.19976436 -0.70392538 -1.55481783 -0.13441475]\n",
      " [-1.27056787  0.77713777  1.92592416  0.59971728]]\n"
     ]
    }
   ],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]  #分子布局\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)  # 矩阵乘法\n",
    "        self.x = x  #保存输入，反向传播时使用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)  # 输入梯度\n",
    "        dW = np.dot(self.x.T, dout)  # 权重梯度\n",
    "        self.grads[0][...] = dW  # 更新梯度\n",
    "        return dx\n",
    "    \n",
    "# 测试 MatMul 类\n",
    "W = np.random.randn(3, 4)  # 假设的权重 \n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "matmul = MatMul(W)  \n",
    "y = matmul.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 4)  # 假设的梯度    \n",
    "dx = matmul.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)  \n",
    "print(\"Weights W:\\n\", W)\n",
    "print(\"Output y:\\n\", y)     \n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)     \n",
    "print(\"Gradient dW:\\n\", matmul.grads[0])  # 权重梯度\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebd6e9",
   "metadata": {},
   "source": [
    "## 局部计算的层\n",
    "\n",
    "通过计算图的计算结点，可以实现一些神经网络的常用层。这些层一般都是结点的组合结果。\n",
    "\n",
    "- **sigmoid层**\n",
    "\n",
    "sigmoid层主要由sigmoid函数组成，即\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "y&=\\frac{1}{\\exp(-x)}\\\\\n",
    "\\frac{\\partial y}{\\partial x}&=y(1-y)\n",
    "\\end{split}\n",
    "$$(sigmoid-layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "663487d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-0.02564088 -2.48760061 -1.30450939]\n",
      " [ 0.2162969  -0.28149647 -1.11479748]]\n",
      "Output y:\n",
      " [[0.49359013 0.07673201 0.21340707]\n",
      " [0.55386439 0.43008693 0.24697757]]\n",
      "Gradient dy:\n",
      " [[-1.6494359   0.5121364  -1.02999527]\n",
      " [ 0.67516035 -0.50797873 -0.81326976]]\n",
      "Gradient dx:\n",
      " [[-0.41229121  0.0362819  -0.17289963]\n",
      " [ 0.1668312  -0.12451177 -0.15125163]]\n"
     ]
    }
   ],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []    \n",
    "        self.grads = []\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))  # Sigmoid 函数\n",
    "        self.out = out  # 保存输出，反向传播时使用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1-self.out)  # Sigmoid 的梯度\n",
    "        return dx\n",
    "\n",
    "# 测试 Sigmoid 类\n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "sigmoid = Sigmoid()\n",
    "y = sigmoid.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 3)  # 假设的梯度\n",
    "dx = sigmoid.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)  # Sigmoid 的梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8cbda",
   "metadata": {},
   "source": [
    "- **仿射层(Affine)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
