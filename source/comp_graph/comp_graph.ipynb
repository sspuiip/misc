{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cb8e12",
   "metadata": {},
   "source": [
    "# 计算图\n",
    "\n",
    "计算图是用于神经网络BP算法的一种工具，其基本思想是复合函数的链式求导法则，可简化误差反向传播的计算。\n",
    "\n",
    "## 局部计算节点\n",
    "\n",
    "计算图将神经网络的推理与学习任务分散各个局部计算节点，通过局部节点的计算结果实现神经网络的推理与学习任务。\n",
    "\n",
    "- **加法节点**\n",
    "\n",
    "加法节点的作用是实现以下推理的计算片断，\n",
    "\n",
    "$$\n",
    "x + y = z\n",
    "$$(node-add)\n",
    "\n",
    "误差的反向传播则将$\\frac{\\partial L}{\\partial z} $乘上以下局部计算结果后，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial z}{\\partial x}&=1\\\\\n",
    "\\frac{\\partial z}{\\partial y}&=1\\\\\n",
    "\\end{split}\n",
    "$$(node-add-local-comp)\n",
    "\n",
    "反向传入相应分支，即，$x,y$的各分支分别反向传入$\\frac{\\partial L}{\\partial z}\\times 1$。式{eq}`node-add-local-comp`分别对应各个分支的局部梯度计算结果。\n",
    "\n",
    "- **乘法节点**\n",
    "\n",
    "与加法节点类似，实现以下局部计算，\n",
    "\n",
    "$$\n",
    "x*y=z\n",
    "$$(node-mult)\n",
    "\n",
    "误差反向传播则分别将以下结果反向传入对应分支，即$x$分支传入，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}=\\frac{\\partial L}{\\partial z}\\cdot y\n",
    "$$(node-mult-back-x)\n",
    "\n",
    "$y$分支传入，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial y}=\\frac{\\partial L}{\\partial z}\\cdot x\n",
    "$$(node-mult-back-y)\n",
    "\n",
    "- **分支节点**\n",
    "\n",
    "分支节点是指相同的值复制后传入各个分支，也称为复制节点。反向传播则是上游传来的梯度之和。当分支扩展到$N$个节点，则可称为**重复节点**。重复节点的反向传播与分支节点类似，是上游所传的梯度之和。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffb40afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-0.80527759 -2.10122899  1.82170405]]\n",
      "Output y:\n",
      " [[-0.80527759 -2.10122899  1.82170405]\n",
      " [-0.80527759 -2.10122899  1.82170405]]\n",
      "Gradient dy:\n",
      " [[-0.15132099 -0.00797295  0.24365406]\n",
      " [ 0.02775551 -0.69924707  1.85117108]]\n",
      "Gradient dx:\n",
      " [[-0.12356548 -0.70722001  2.09482514]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(1, 3)  # 假设的输入数据\n",
    "y = np.repeat(x, 2, axis=0) # 正向传播\n",
    "\n",
    "dy=np.random.randn(2, 3) #假设的梯度\n",
    "dx=np.sum(dy, axis=0, keepdims=True) #梯度传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y) \n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227fe11",
   "metadata": {},
   "source": [
    "- **sum节点**\n",
    "\n",
    "sum节点与重复节点正好相反，推理时其输入为各个分支的和，反向传播时各分支传入的值是其上游值的复制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "271a22d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 0.08492191 -0.91717507 -1.23947848]\n",
      " [-0.86875288  0.46808049  0.57779055]]\n",
      "Output y:\n",
      " [[-0.78383097 -0.44909459 -0.66168793]]\n",
      "Gradient dy:\n",
      " [[ 0.95962873 -0.21590781  0.19060428]]\n",
      "Gradient dx:\n",
      " [[ 0.95962873 -0.21590781  0.19060428]\n",
      " [ 0.95962873 -0.21590781  0.19060428]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "y = np.sum(x, axis=0, keepdims=True)  # 正向传播\n",
    "dy = np.random.randn(1, 3)  # 假设的梯度\n",
    "dx = np.repeat(dy, 2, axis=0)  # 梯度传播\n",
    "print(\"Input x:\\n\", x)  \n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad906ccf",
   "metadata": {},
   "source": [
    "- **MatMul节点**\n",
    "\n",
    "矩阵乘积节点(假设向量为行向量)，即\n",
    "\n",
    "$$\n",
    "\\pmb{y}_{1\\times H}=\\pmb{x}_{1\\times D}\\pmb{W}_{D\\times H}\n",
    "$$(node-matmul)\n",
    "\n",
    "该计算结点的难点在于反向传播，即以下偏导数(**分子布局**)的计算，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial L}{\\partial \\pmb{x}}_{1\\times D}&=\\frac{\\partial L}{\\partial \\pmb{y}}\\frac{\\partial \\pmb{y}}{\\partial \\pmb{x}}   =\\left[\\frac{\\partial L}{\\pmb{y}}\\right]_{1\\times H}\\left[\\pmb{W}^\\top\\right]_{H\\times D}\\\\\n",
    "\\left[\\frac{\\partial L}{\\partial \\pmb{W}}\\right]_{D\\times H}&=\\frac{\\partial L}{\\partial \\pmb{y}}\\frac{\\partial\\pmb{y}}{\\partial \\pmb{W}}   =\\left[\\pmb{x}^\\top\\right]_{D\\times 1} \\left[\\frac{\\partial L}{\\partial\\pmb{y}}\\right]_{1\\times H}\\\\\n",
    "\\end{split}\n",
    "$$(node-matmul-back)\n",
    "\n",
    "式{eq}`node-matmul-back`的第1个等式容易实现，略过。第2个等式的推导如下：由矩阵乘法定义可知，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_j}{\\partial W_{ik}}=\\left\\{\\begin{array}{ll}x_i,&j==k\\\\ 0,& j\\neq k \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "因此，可以得到以下等式，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{ij}}=\\sum_k \\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial y_j}\\frac{\\partial y_j}{\\partial W_{ij}}=\\frac{\\partial L}{\\partial y_j}\\cdot x_i\n",
    "$$\n",
    "\n",
    "则有梯度如下，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\pmb{W}}=\\left[ \\frac{\\partial L}{\\partial y_j}x_i  \\right]_{ij}=\\left[\\pmb{x}^\\top\\right]_{D\\times 1} \\left[\\frac{\\partial L}{\\partial\\pmb{y}}\\right]_{1\\times H}\n",
    "$$\n",
    "\n",
    "注意：当$\\pmb{x}$是小批量样本时，{eq}`node-matmul-back`形式仍然保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8758672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 0.03352755  1.18486835 -1.01735471]\n",
      " [ 0.32189715  0.26699387  0.31725705]]\n",
      "Weights W:\n",
      " [[-0.55791106 -0.9511502   0.74444144  1.53739171]\n",
      " [ 1.36321165  0.14117636 -0.44070922 -1.09464207]\n",
      " [-1.0261097  -0.73398957  0.87890245 -0.29464672]]\n",
      "Output y:\n",
      " [[ 2.64043849  0.88211341 -1.39137866 -0.94570154]\n",
      " [-0.14116137 -0.50134268  0.40080492  0.10914054]]\n",
      "Gradient dy:\n",
      " [[ 1.38707728 -2.50204509  0.90305722  0.00865469]\n",
      " [ 2.03472731  0.03831168 -1.19331181 -0.85455156]]\n",
      "Gradient dx:\n",
      " [[ 2.2915338   1.13019084  1.20433068]\n",
      " [-3.37376827  4.24050429 -2.91298765]]\n",
      "Gradient dW:\n",
      " [[ 0.70147823 -0.07155502 -0.35384637 -0.27478754]\n",
      " [ 2.18676369 -2.95436506  0.75139699 -0.21790536]\n",
      " [-0.765618    2.55762199 -1.29731611 -0.2799174 ]]\n"
     ]
    }
   ],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]  #分子布局\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)  # 矩阵乘法\n",
    "        self.x = x  #保存输入，反向传播时使用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)  # 输入梯度\n",
    "        dW = np.dot(self.x.T, dout)  # 权重梯度\n",
    "        self.grads[0][...] = dW  # 更新梯度\n",
    "        return dx\n",
    "    \n",
    "# 测试 MatMul 类\n",
    "W = np.random.randn(3, 4)  # 假设的权重 \n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "matmul = MatMul(W)  \n",
    "y = matmul.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 4)  # 假设的梯度    \n",
    "dx = matmul.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)  \n",
    "print(\"Weights W:\\n\", W)\n",
    "print(\"Output y:\\n\", y)     \n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)     \n",
    "print(\"Gradient dW:\\n\", matmul.grads[0])  # 权重梯度\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebd6e9",
   "metadata": {},
   "source": [
    "## 局部计算的层\n",
    "\n",
    "通过计算图的计算结点，可以实现一些神经网络的常用层。这些层一般都是结点的组合结果。\n",
    "\n",
    "- **sigmoid层**\n",
    "\n",
    "sigmoid层主要由sigmoid函数组成，即\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "y&=\\frac{1}{\\exp(-x)}\\\\\n",
    "\\frac{\\partial y}{\\partial x}&=y(1-y)\n",
    "\\end{split}\n",
    "$$(sigmoid-layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663487d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[-1.65838602  1.02453508 -0.30456766]\n",
      " [-1.623109   -1.06858129  0.33342183]]\n",
      "Output y:\n",
      " [[0.15997877 0.73585504 0.42444126]\n",
      " [0.16477655 0.25567298 0.58259173]]\n",
      "Gradient dy:\n",
      " [[ 0.06297039  1.46850255  1.18610748]\n",
      " [-0.21990096 -0.63435627  0.57927704]]\n",
      "Gradient dx:\n",
      " [[ 0.00846231  0.28543637  0.28975524]\n",
      " [-0.03026392 -0.12072073  0.14086778]]\n"
     ]
    }
   ],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []    \n",
    "        self.grads = []\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))  # Sigmoid 函数\n",
    "        self.out = out  # 保存输出，反向传播时使用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1-self.out)  # Sigmoid 的梯度\n",
    "        return dx\n",
    "\n",
    "# 测试 Sigmoid 类\n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "sigmoid = Sigmoid()\n",
    "y = sigmoid.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 3)  # 假设的梯度\n",
    "dx = sigmoid.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)  # Sigmoid 的梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8cbda",
   "metadata": {},
   "source": [
    "- **仿射层(Affine)**\n",
    "\n",
    "Affine层主要实现了线性计算的功能，通过矩阵乘法节点和重复节点完成计算功能。\n",
    "\n",
    "$$\n",
    "\\pmb{z}=\\pmb{x}^\\top\\pmb{W}+\\pmb{b}\n",
    "$$(affine-node)\n",
    "\n",
    ":::{figure-md}\n",
    ":name: fig-affine\n",
    "![仿射节点](../img/affine.svg){width=600px}\n",
    "\n",
    "仿射节点\n",
    ":::\n",
    "\n",
    "参见图{ref}`fig-affine`的计算过程。该层的计算代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ab064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 0.88704974 -0.30459424  0.49883379]\n",
      " [-0.93483483 -1.51573514  1.48494939]]\n",
      "Weights W:\n",
      " [[-1.07665424  0.04425788  0.35114002  0.13897377]\n",
      " [-1.80324798  1.1501323   0.05891955  0.71797076]\n",
      " [ 0.52208198 -0.04464385 -0.96533174 -0.11528565]]\n",
      "Bias b:\n",
      " [ 1.12997141 -0.82246972  1.78742753 -1.235317  ]\n",
      "Output y:\n",
      " [[ 0.98461663 -1.15580432  1.59941955 -1.38823849]\n",
      " [ 5.64497694 -2.67343332 -0.0636056  -2.62468139]]\n",
      "Gradient dy:\n",
      " [[-0.24752332 -1.01983307  0.9752888   0.84968113]\n",
      " [-0.31528368 -0.48409676  0.31856986  0.61283347]]\n",
      "Gradient dx:\n",
      " [[ 0.6819077  -0.05908725 -1.12313146]\n",
      " [ 0.51505682  0.47052585 -0.52116849]]\n",
      "Gradient dW:\n",
      " [[ 0.07517267 -0.45209215  0.56731947  0.18081135]\n",
      " [ 0.55328073  1.04439775 -0.77993489 -1.1877012 ]\n",
      " [-0.5916533  -1.22758638  0.95956713  1.33387634]]\n",
      "Gradient db:\n",
      " [-0.562807   -1.50392983  1.29385866  1.4625146 ]\n"
     ]
    }
   ],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None   #反向传播时需要\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dw = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...]=dw\n",
    "        self.grads[1][...]=db\n",
    "        return dx \n",
    "\n",
    "#测试Affine节点\n",
    "W = np.random.randn(3, 4)  # 假设的权重\n",
    "b = np.random.randn(4)  # 假设的偏置    \n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "affine = Affine(W, b)\n",
    "y = affine.forward(x)  # 正向传播\n",
    "dy = np.random.randn(2, 4)  # 假设的梯度\n",
    "dx = affine.backward(dy)  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Weights W:\\n\", W)\n",
    "print(\"Bias b:\\n\", b)\n",
    "print(\"Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)\n",
    "print(\"Gradient dx:\\n\", dx)  # 输入梯度\n",
    "print(\"Gradient dW:\\n\", affine.grads[0])  # 权重梯度\n",
    "print(\"Gradient db:\\n\", affine.grads[1])  # 偏置梯度\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1b8e2",
   "metadata": {},
   "source": [
    "- **Softmax损失层**\n",
    "\n",
    "该层主要由softmax函数以及交叉熵损失函数复合而成。softmax函数是指以下函数，\n",
    "\n",
    "$$\n",
    "softmax(\\pmb{x})=\\frac{\\exp(\\pmb{x})}{\\sum_i \\exp(x_i)}\n",
    "$$(softmax-fun-def)\n",
    "\n",
    "交叉熵损失则是指以下损失函数(单个样本one-hot形式)，\n",
    "\n",
    "$$\n",
    "loss(y,t)=\\sum_i t_i\\log y_i\n",
    "$$(corss-entropy-def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dee384b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[ 0.         -1.59407251 -0.47112775]\n",
      " [-2.35106312  0.         -3.12348144]]\n",
      "Labels t:\n",
      " [0 1]\n",
      "Softmax Output y:\n",
      " [[0.54722717 0.11114009 0.34163273]\n",
      " [0.0836217  0.87775387 0.03862442]]\n",
      "Gradient dy:\n",
      " [[-0.22638641  0.05557005  0.17081637]\n",
      " [ 0.04181085 -0.06112306  0.01931221]]\n",
      "Cross Entropy Loss:\n",
      " 0.36664000348434594\n",
      "Gradient dx:\n",
      " [[-0.22638641  0.05557005  0.17081637]\n",
      " [ 0.04181085 -0.06112306  0.01931221]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:  # 如果是二维数组\n",
    "        x -= np.max(x, axis=1, keepdims=True)  # 减去每行的最大值\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)  # softmax计算\n",
    "    else:  # 如果是一维数组\n",
    "        x -= np.max(x)  # 减去最大值\n",
    "        y = np.exp(x) / np.sum(np.exp(x))  # softmax计算\n",
    "    return y\n",
    "\n",
    "def cross_entropy_loss(y, t):\n",
    "    if y.ndim == 1:  # 如果是向量\n",
    "        y = y.reshape(1, -1)  # 转换为二维数组\n",
    "        t = t.reshape(1, -1)  # 转换为二维数组\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)  # 如果t是one-hot编码，转换为类别索引\n",
    "    batch_size = y.shape[0]  # 获取批大小\n",
    "    loss = -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size  # 计算交叉熵损失\n",
    "    return loss\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None  # 保存softmax输出\n",
    "        self.t = None  # 保存标签\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t  # 保存标签\n",
    "        self.y = softmax(x)  # 计算softmax输出\n",
    "\n",
    "        # 在监督标签为one-hot向量的情况下，转换为正确解标签的索引\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        loss = cross_entropy_loss(self.y, self.t)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.y.shape[0]  # 获取批大小\n",
    "        dx = self.y.copy()  # 复制softmax输出\n",
    "        dx[np.arange(batch_size), self.t] -= 1  # 减去正确类别的梯度\n",
    "        dx *= dout\n",
    "        dx /= batch_size  # 平均化梯度\n",
    "        return dx\n",
    "    \n",
    "# 测试 SoftmaxWithLoss 类\n",
    "x = np.random.randn(2, 3)  # 假设的输入数据\n",
    "t = np.array([0, 1])  # 假设的标签  \n",
    "softmax_loss = SoftmaxWithLoss()\n",
    "y = softmax_loss.forward(x, t)  # 正向传播\n",
    "dy = softmax_loss.backward()  # 反向传播\n",
    "print(\"Input x:\\n\", x)\n",
    "print(\"Labels t:\\n\", t)\n",
    "print(\"Softmax Output y:\\n\", y)\n",
    "print(\"Gradient dy:\\n\", dy)  # softmax的梯度\n",
    "# 测试 SoftmaxWithLoss 类的交叉熵损失\n",
    "loss = cross_entropy_loss(y, t)  # 计算交叉熵损失\n",
    "print(\"Cross Entropy Loss:\\n\", loss)  # 输出交叉熵损失\n",
    "# 测试 SoftmaxWithLoss 类的梯度\n",
    "print(\"Gradient dx:\\n\", dy)  # 输出梯度\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
